# -*- coding: utf-8 -*-
"""Econometrics_&_Forecasting_Final_Draft.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQX6NMb6dgktpZX2TFX0E4KV6BQS4Lez

## Importing and Basic Checks
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import datetime
import plotly.express as px

# This function reads a CSV file and tries to automatically convert columns
# that look like numbers (e.g. "1000", "45.5") into proper numeric types (int or float).
# It helps clean the data so numeric values don't stay as strings by mistake.
# Columns that can't be converted are left as they are.

def read_and_convert_csv(filepath):
    df = pd.read_csv(filepath)

    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # Attempt conversion to numeric
                numeric_col = pd.to_numeric(df[col], errors='coerce')

                # Proceed if most values convert successfully
                if numeric_col.notna().mean() > 0.8:
                    if (numeric_col.dropna() % 1 == 0).all():
                        df[col] = numeric_col.astype('Int64')  # Safe integer type
                    else:
                        df[col] = numeric_col.astype(float)
            except:
                pass  # Leave column unchanged if conversion fails

    return df

FinancialLiteracyNational = read_and_convert_csv('/content/Econometrics & Forecasting - Andreea.csv')

print(FinancialLiteracyNational)

# Shows the number of missing values per column
FinancialLiteracyNational.isnull().sum()

FinancialLiteracyNational.duplicated().any()

TECH = read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (TECH).csv')

print(TECH)

# Shows the number of missing values per column
TECH.isnull().sum()

TECH.duplicated().any()

HEALTHCARE = read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (HEALTHCARE).csv')

print(HEALTHCARE)

# Shows the number of missing values per column
HEALTHCARE.isnull().sum()

HEALTHCARE.duplicated().any()

HEALTHCARE.isnull().sum()

FINANCE = read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (FINANCE).csv')

print(FINANCE)

# Shows the number of missing values per column
FINANCE.isnull().sum()

FINANCE.duplicated().any()

FINANCE.isnull().sum()

CONSUMERDISCRETIONARY = read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (CONSUMER DISCRETIONARY).csv')

print(CONSUMERDISCRETIONARY)

# Shows the number of missing values per column
CONSUMERDISCRETIONARY.isnull().sum()

CONSUMERDISCRETIONARY.duplicated().any()

CONSUMERDISCRETIONARY.head(5)

#data check

COMMUNICATIONS = read_and_convert_csv('/content/Econometrics & Forecasting - Ayaka Communication.csv')

print(COMMUNICATIONS)

COMMUNICATIONS.head(5)
#data check

# Shows the number of missing values per column
COMMUNICATIONS.isnull().sum()

COMMUNICATIONS.duplicated().any()

AI = read_and_convert_csv('/content/Econometrics & Forecasting - Ayaka AI.csv')

print(AI)

# Shows the number of missing values per column
AI.isnull().sum()

AI.duplicated().any()

"""## Standardising Data Format/Cleaning


"""

#  Rename columns consistently
COMMUNICATIONS.rename(columns={
        'symbol': 'Symbol',
        'date': 'Date',
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume',
        'adjusted': 'Adjusted Close'
    }, inplace=True)

AI.rename(columns={
        'symbol': 'Symbol',
        'date': 'Date',
        'open': 'Open',
        'high': 'High',
        'low': 'Low',
        'close': 'Close',
        'volume': 'Volume',
        'adjusted': 'Adjusted Close'
    }, inplace=True)

FinancialLiteracyNational.rename(columns={
        'age': 'Age',
        'gender': 'Gender',
        'Irish region': 'Irish Region',
        'income': 'Income',
        'financial literacy': 'Financial Literacy',
        'risk aversion': 'Risk Aversion',
        'do you invest': 'Do You Invest?',
        'what sector(s) would you invest in':'What Sector(s) Would You Invest In?',
    }, inplace=True)

print(FinancialLiteracyNational)

FinancialLiteracyNational[50:100]

FinancialLiteracyNational["Do You Invest?"] = FinancialLiteracyNational["Do You Invest?"].replace("no", "No")
#standardising data

FinancialLiteracyNational["Do You Invest?"] = FinancialLiteracyNational["Do You Invest?"].replace("yes", "Yes")

FinancialLiteracyNational["Risk Aversion"] = FinancialLiteracyNational["Risk Aversion"].replace("risk neutral", "Risk Neutral")

FinancialLiteracyNational["Risk Aversion"] = FinancialLiteracyNational["Risk Aversion"].replace("risk averse", "Risk Averse")

FinancialLiteracyNational["Risk Aversion"] = FinancialLiteracyNational["Risk Aversion"].replace("risk loving", "Risk Loving")

print(AI)

print(COMMUNICATIONS)

Communications= COMMUNICATIONS.dropna()

Communications.isnull().sum()

print(Communications)

# Drop missing rows and make a copy to avoid SettingWithCopyWarning
Communications = COMMUNICATIONS.dropna().copy()

Communications.rename(columns={
        'adjclose': 'Adjclose',
    }, inplace=True)

# Convert 'Date' column to datetime objects
Communications['Date'] = pd.to_datetime(Communications['Date'])

# Format the dates as DD/MM/YYYY
Communications['Date'] = Communications['Date'].dt.strftime('%d/%m/%Y')

print(Communications)

AI['Date'] = pd.to_datetime(AI['Date'])

# Format the dates as DD/MM/YYYY
AI['Date'] = AI['Date'].dt.strftime('%d/%m/%Y')

print(AI)

# # Drop the 'Adjusted Close' column
# AI = AI.drop('Adjusted Close', axis=1)

print(Communications)

#  Convert categorical variables into structured format #Structure categorical data
def convert_to_categorical(df):  # Define a function that takes a DataFrame as input
    categorical_cols = ['Country', 'Sector', 'Gender']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = pd.Categorical(df[col])
    return df # Return the modified DataFrame

def round_numeric_data(df): #Rounds numeric data to 2 decimal places
    """Rounds numeric data to 2 decimal places."""
    df = df.round(2)
    return df

def clean_dataframe(df):
    df.columns = df.columns.str.strip()
    df = df.dropna(axis=1, how='all')
    df = convert_to_categorical(df)
    df = round_numeric_data(df)
    return df

# Load and clean datasets for the project

# 1. Load Financial Literacy dataset (Andreea only)
FinancialLiteracy = clean_dataframe(
    read_and_convert_csv('/content/Econometrics & Forecasting - Andreea.csv')
)

# 2. Load and combine all stock-related datasets (Ayaka + Shanice)
STOCKPRICES = pd.concat([
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Ayaka AI.csv')),
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Ayaka Communication.csv')),
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (TECH).csv')),
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (HEALTHCARE).csv')),
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (FINANCE).csv')),
    clean_dataframe(read_and_convert_csv('/content/Econometrics & Forecasting - Shanice (CONSUMER DISCRETIONARY).csv')),
], axis=0)

"""#Merge Datasets

"""

print(AI)

print(Communications)

print(TECH)

Communications.rename(columns={
        'Open': 'Open Price',
        'High': 'High Price',
        'Low': 'Low Price',
        'Close': 'Close Price',
        'Company': 'Company Name',
    }, inplace=True)

print(Communications)

AI.rename(columns={
        'Open': 'Open Price',
        'High': 'High Price',
        'Low': 'Low Price',
        'Close': 'Close Price',
        'Company': 'Company Name',
    }, inplace=True)

print(AI)

AI.rename(columns={
        'Symbol': 'Company Name',
    }, inplace=True)

AI["Company Name"] = AI["Company Name"].replace("NVDA", "Nvidia")
AI["Company Name"] = AI["Company Name"].replace("META", "Meta")
AI["Company Name"] = AI["Company Name"].replace("ANET", "Arista Networks Inc")
AI["Company Name"] = AI["Company Name"].replace("AMZN", "Amazon")
AI["Company Name"] = AI["Company Name"].replace("PANW", "Palo Alto Networks Inc")
AI["Company Name"] = AI["Company Name"].replace("GOOG", "Google")
AI["Company Name"] = AI["Company Name"].replace("TSLA", "Tesla")
AI["Company Name"] = AI["Company Name"].replace("NOW", "ServiceNow Inc")
AI["Company Name"] = AI["Company Name"].replace("AMD", "Advanced Micro Devices Inc")
AI["Company Name"] = AI["Company Name"].replace("PATH", "UiPath Inc")
AI["Company Name"] = AI["Company Name"].replace("AI", "C3.ai Inc")

print(AI)

FINANCE.drop('Symbol', axis=1, inplace=True)
CONSUMERDISCRETIONARY.drop('Symbol', axis=1, inplace=True)
HEALTHCARE.drop('Symbol', axis=1, inplace=True)

# prompt: add a new column to the FINANCE dataset called Sector. The rows under Sector should be Finance

FINANCE['Sector'] = 'Finance'

TECH['Sector'] = 'Tech'

HEALTHCARE['Sector'] = 'Healthcare'

CONSUMERDISCRETIONARY['Sector'] = 'Consumer Discretionary'

Communications['Sector'] = 'Communications'

AI['Sector'] = 'AI'

STOCKPRICES = pd.concat([AI, Communications, TECH, HEALTHCARE, FINANCE, CONSUMERDISCRETIONARY], axis=0)

print(STOCKPRICES)

"""#Descriptive analysis and visualisations

"""

FinancialLiteracyNational.info()

# Normalise column names
FinancialLiteracyNational.columns = (
    FinancialLiteracyNational.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

print(FinancialLiteracyNational['do you invest'].value_counts())

FinancialLiteracyNational.columns = (
    FinancialLiteracyNational.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

print(FinancialLiteracyNational['what sectors would you invest in'].value_counts())

FINANCE.columns = (
    FINANCE.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

print(FINANCE['close price'].value_counts())

CONSUMERDISCRETIONARY.columns = (
    CONSUMERDISCRETIONARY.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

print(CONSUMERDISCRETIONARY['close price'].value_counts())

CONSUMERDISCRETIONARY.columns = (
    CONSUMERDISCRETIONARY.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

CONSUMERDISCRETIONARY.info()

HEALTHCARE.columns = (
    HEALTHCARE.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

HEALTHCARE.info()

TECH.columns = (
    TECH.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

TECH.info()

AI.info()

COMMUNICATIONS.columns = (
    COMMUNICATIONS.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

COMMUNICATIONS.info()

FinancialLiteracyNational.describe()

HEALTHCARE.columns = (
    HEALTHCARE.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

HEALTHCARE.describe()

TECH.columns = (
    TECH.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)
TECH.describe()

FINANCE.columns = (
    FINANCE.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)
FINANCE.describe()

CONSUMERDISCRETIONARY.columns = (
    CONSUMERDISCRETIONARY.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)
CONSUMERDISCRETIONARY.describe()

COMMUNICATIONS.columns = (
    COMMUNICATIONS.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)
COMMUNICATIONS.describe()

AI.describe()

"""## User demographics and financial literacy

"""

#Histograms showing how age, income, and financial literacy scores are distributed
FinancialLiteracyNational[['age', 'income', 'financial literacy']].hist(bins=20, figsize=(12, 5), edgecolor='black')
plt.suptitle('Distributions of Age, Income, and Financial Literacy', fontsize=14)
plt.tight_layout()
plt.show()

#Bar chart that comparing how many males vs. females invest
plt.figure(figsize=(6,4))
sns.countplot(x='gender', hue='do you invest', data=FinancialLiteracyNational)
plt.title("Investment Interest by Gender")
plt.ylabel("Count")
plt.xlabel("Gender")
plt.legend(title="Do you invest?")
plt.tight_layout()
plt.show()

# 1. KDE Plot – Financial Literacy Distribution
plt.figure(figsize=(6, 4))
sns.kdeplot(FinancialLiteracyNational['financial literacy'], fill=True, color='skyblue')
plt.title("Financial Literacy – Distribution")
plt.xlabel("Score")
plt.tight_layout()
plt.show()

# 2. Bar Plot – Investment Status by Region
FinancialLiteracyNational.columns = (
    FinancialLiteracyNational.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

# Now create the bar plot using the cleaned column names
plt.figure(figsize=(12, 5))
region_invest = FinancialLiteracyNational.groupby(['irish region', 'do you invest']).size().unstack().fillna(0)
region_invest.plot(kind='bar', stacked=True, colormap='Set2')
plt.title("Investment by Region")
plt.ylabel("Number of Students")
plt.xlabel("Region")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#Donut chart that shows what percentage of users answered "Yes" or "No" to the "Do you invest?" question.
import plotly.express as px

fig = px.pie(FinancialLiteracyNational,
             names='do you invest',  # Changed column name to match the DataFrame
             title='Investment Participation Among Users',
             hole=0.4)  # makes it a donut chart
fig.show()

#Donut chart showing the distribution of financial literacy levels
import plotly.express as px

fig = px.pie(FinancialLiteracyNational,
             names='financial literacy',  # Changed column name to match the DataFrame
             title='Financial Literacy Among Users',
             hole=0.4)  # makes it a donut chart
fig.show()

"""## Sector market trends

"""

# Assuming 'Date' is a datetime column
FINANCE['date'] = pd.to_datetime(FINANCE['date'], format='%d/%m/%Y') # Changed format to '%d/%m/%Y'
FINANCE = FINANCE.set_index('date')

# Resample data monthly and calculate the mean close price
FINANCE_resampled = FINANCE['close price'].resample('M').mean() # Corrected column name to 'close price'

# Plot the resampled data
fig = px.line(FINANCE_resampled, title='Finance Stock Price Over Time')
fig.show()

# Assuming 'Date' is a datetime column
HEALTHCARE['date'] = pd.to_datetime(HEALTHCARE['date'], format='%d/%m/%Y') # Changed format to '%d/%m/%Y'
HEALTHCARE = HEALTHCARE.set_index('date')

# Resample data monthly and calculate the mean close price
HEALTHCARE_resampled = HEALTHCARE['close price'].resample('M').mean() # Corrected column name to 'close price'

# Plot the resampled data
fig = px.line(HEALTHCARE_resampled, title='Healthcare Stock Price Over Time')
fig.show()

# Assuming 'Date' is a datetime column
TECH['date'] = pd.to_datetime(TECH['date'], format='%d/%m/%Y') # Changed format to '%d/%m/%Y' and column name to 'date'
TECH = TECH.set_index('date') #This line may also cause an error so changed 'Date' to 'date'

# Resample data monthly and calculate the mean close price
TECH_resampled = TECH['close price'].resample('M').mean() # Corrected column name to 'close price'

# Plot the resampled data
fig = px.line(TECH_resampled, title='Tech Stock Price Over Time')
fig.show()

# Assuming 'Date' is a datetime column
CONSUMERDISCRETIONARY['date'] = pd.to_datetime(CONSUMERDISCRETIONARY['date'], format='%d/%m/%Y') # Changed format to '%d/%m/%Y' and column name to 'date'
CONSUMERDISCRETIONARY = CONSUMERDISCRETIONARY.set_index('date') #This line may also cause an error so changed 'Date' to 'date'

# Resample data monthly and calculate the mean close price
CONSUMERDISCRETIONARY_resampled = CONSUMERDISCRETIONARY['close price'].resample('M').mean() # Corrected column name to 'close price'

# Plot the resampled data
fig = px.line(CONSUMERDISCRETIONARY_resampled, title='Consumer Discretionary Stock Price Over Time')
fig.show()

# Clean column names
AI.columns = (
    AI.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

# Fix date format
AI['date'] = pd.to_datetime(AI['date'], errors='coerce')

# Create line chart
fig = px.line(
    AI,
    x='date',
    y='close price',
    color='company name',
    title='AI Stock Prices Over Time'
)

# Improve x-axis readability
fig.update_layout(xaxis=dict(tickformat="%Y-%m", tickangle=45))

fig.show()

# Compare how different communication company stock prices have moved over time

COMMUNICATIONS.columns = (
    COMMUNICATIONS.columns
    .str.strip()
    .str.lower()
    .str.replace(r'[^\w\s]', '', regex=True)
)

fig = px.line(
    COMMUNICATIONS,
    x='date',
    y='close price',
    color='company',
    title='Communication Stock Prices Over Time'
)

fig.show()

"""# Regression

"""

print(FinancialLiteracyNational)

import statsmodels.api as sm

gender_mapping = {'M': 0, 'F': 1}
FinancialLiteracyNational['gender'] = FinancialLiteracyNational['gender'].map(gender_mapping)

risk_mapping = {'Risk Loving': 0, 'Risk Neutral': 1, 'Risk Averse': 2}
FinancialLiteracyNational['risk aversion'] = FinancialLiteracyNational['risk aversion'].map(risk_mapping)

investment_mapping = {'Yes': 1, 'No': 0}
FinancialLiteracyNational['do you invest'] = FinancialLiteracyNational['do you invest'].map(investment_mapping)

print(FinancialLiteracyNational)

# Use normalized column names
y = FinancialLiteracyNational['financial literacy']
X = FinancialLiteracyNational['age']
X = sm.add_constant(X)

# Fit and print model
model_single = sm.OLS(y, X).fit()
print(model_single.summary())

y = FinancialLiteracyNational['financial literacy']
X = FinancialLiteracyNational['gender']
X = sm.add_constant(X)

model_single = sm.OLS(y, X).fit()
print(model_single.summary())

y = FinancialLiteracyNational['financial literacy']
X = FinancialLiteracyNational['do you invest']
X = sm.add_constant(X)

model_single = sm.OLS(y, X).fit()
print(model_single.summary())

print(FinancialLiteracyNational[['financial literacy', 'gender', 'age', 'risk aversion', 'do you invest']].isna().sum())

# # Target variable
y = FinancialLiteracyNational['financial literacy']

# # Predictor variables with normalized names
X = FinancialLiteracyNational[['gender', 'age', 'risk aversion', 'do you invest']]
X = sm.add_constant(X)

# # Fit and summarize model
model_mult = sm.OLS(y, X).fit()
print(model_mult.summary())

print(STOCKPRICES)

cols_to_drop = [col for col in STOCKPRICES.columns if col.strip().lower() in ['symbol', 'adjclose']]
STOCKPRICES.drop(columns=cols_to_drop, inplace=True)

cols_to_drop = [col for col in STOCKPRICES.columns if 'adjusted' in col.lower()]
STOCKPRICES.drop(columns=cols_to_drop, inplace=True)

print(STOCKPRICES)

y = STOCKPRICES['Close Price']
X = STOCKPRICES[['High Price','Low Price','Open Price','Volume']]
X = sm.add_constant(X)
model_mult = sm.OLS(y, X).fit()
print(model_mult.summary())

"""# Forecasting"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Load the CSV
nvda_df = pd.read_csv("Econometrics & Forecasting - Ayaka AI.csv")

# Prepare the data
nvda_df["Date"] = pd.to_datetime(nvda_df["Date"])
nvda_df.set_index("Date", inplace=True)
nvda_df = nvda_df[["Adjusted"]].dropna()

# Use the most recent 200 rows
nvda_recent = nvda_df[-200:]
nvda_recent = nvda_recent[~nvda_recent.index.duplicated(keep='first')]

# Optional: Visual check
nvda_recent.tail()

# The MA(2) forecast appears flat because moving average models rely on past error terms, not past values.
# Once the forecast period starts, the model assumes future errors are zero, leading to a flat forecast line.
# This makes MA models less effective for forecasting trending data like stock prices.
# They are better used for modeling noise or residuals in stationary series.
# For upward-trending assets like stocks, AR or ARMA models are generally more suitable.

# Make sure nvda_recent exists and has 'Adjusted' and datetime index
# Example: nvda_recent = STOCKPRICES[STOCKPRICES['company'] == 'NVIDIA'].set_index('date')

import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Forecast next 252 business days (approx. 1 year)
forecast_steps = 252

# Fit MA(2) model
ma_model = ARIMA(nvda_recent['Adjusted'], order=(0, 0, 2))  # AR=0, I=0, MA=2
ma_result = ma_model.fit()

# Summary
print(ma_result.summary())

# Forecast
ma_forecast = ma_result.forecast(steps=forecast_steps)

# Plotting
plt.figure(figsize=(15, 6))
plt.plot(nvda_recent.index, nvda_recent["Adjusted"], label="Actual")
plt.plot(pd.date_range(nvda_recent.index[-1] + pd.Timedelta(days=1), periods=forecast_steps, freq="B"),
         ma_forecast, label="MA(2) Forecast", linestyle="--", color="orange")
plt.title("MA(2) Forecast for NVIDIA Adjusted Stock Price (Next 252 Days)")
plt.xlabel("Date")
plt.ylabel("Adjusted Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Why the AR(2) forecast looks flat:
# Autoregressive models (like AR(2)) rely only on past values, so after the recent trend ends,
# the model expects stability or gradual return to recent levels based on lag structure.
# AR(2) does not account for growth trends or seasonality — it’s purely driven by lagged values.
# This flat behavior is typical, especially when no differencing (d=0) or exogenous variables are used.

import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
import pandas as pd

# Fit AR(2) model to NVIDIA adjusted prices
ar_model = ARIMA(nvda_recent['Adjusted'], order=(2, 0, 0))
ar_result = ar_model.fit()

# Print model summary
print(ar_result.summary())

# Forecast 1 year ahead (~252 business days)
forecast_steps = 252
ar_forecast = ar_result.forecast(steps=forecast_steps)

# Build future date index (excluding current last date)
future_dates = pd.date_range(start=nvda_recent.index[-1], periods=forecast_steps + 1, freq='B')[1:]

# Plot actual + forecast
plt.figure(figsize=(16, 6))
plt.plot(nvda_recent.index, nvda_recent['Adjusted'], label="Actual")
plt.plot(future_dates, ar_forecast, label="AR(2) Forecast (1 Year)", linestyle="--", color="green")
plt.title("1-Year AR(2) Forecast for NVIDIA Adjusted Stock Price")
plt.xlabel("Date")
plt.ylabel("Adjusted Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Why ARMA(2,2) forecast so linear:

# No differencing (d=0): The model assumes the data is stationary (no trend), so it doesn't learn long-term upward/downward movement.

# Short forecast horizon (1 year): The model projects based on recent momentum and often "flattens out" when uncertainty increases.

# No seasonality modeled: ARMA doesn’t account for cycles or repeating patterns, so the result is smoother.

# Low recent volatility: If the last part of your actual data was steady or trending slowly, the model just extends that.

# Model simplicity: ARMA(2,2) only looks at recent lags (2 AR, 2 MA terms), so it’s limited in complexity and often outputs a basic trend.



# Fit ARMA(2,2) model on nvda_recent
arma_model = ARIMA(nvda_recent, order=(2, 0, 2))  # AR=2, I=0, MA=2
arma_result = arma_model.fit()

# Summary
print(arma_result.summary())

# Forecasting the next 252 business days (~1 year)
arma_forecast = arma_result.forecast(steps=252)

# Plot actual and forecasted values
plt.figure(figsize=(12, 6))
plt.plot(nvda_recent.index, nvda_recent["Adjusted"], label="Actual")
plt.plot(pd.date_range(nvda_recent.index[-1], periods=253, freq="B")[1:], arma_forecast,
         label="ARMA(2,2) Forecast (1 Year)", linestyle="--", color="orange")

plt.title("1-Year ARMA(2,2) Forecast for NVIDIA Adjusted Stock Price")
plt.xlabel("Date")
plt.ylabel("Adjusted Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
